"""
HTTP Directory keywords

(C) Copyright 2015-2017 Hewlett Packard Enterprise Development LP
"""

from shutil import copyfileobj
import threading
from functools import wraps
import time
import datetime
import os
import unittest
import re
from multiprocessing.pool import ThreadPool
import requests
from lxml import html
from dateutil.parser import parse
from RoboGalaxyLibrary.utilitylib import logging as logger

def connected(decorated):
    """
    Decorator which ensures that the HTTP Server is connected before running a method
    """
    @wraps(decorated)
    def wrapper(self, *args, **kwargs):
        if self.session is None:
            raise AssertionError('Not logged into Http Server')
        return decorated(self, *args, **kwargs)
    return wrapper


class HttpDirectoryKeywords(object):

    """
    HTTP Directory keywords
    """

    class HttpFile(object):

        """
        HTTP File Object
        """
        url = ""
        name = ""
        size = ""
        date = ""

        def __init__(self, name):
            self.name = name.strip('/')

    def __init__(self):
        self.session = None
        self.last_modified = ''
        self.parentdirectory = ''

    @staticmethod
    def get_file_from_local_path(local_path, pattern='.*'):
        """
           Return latest file from local path
        """
        base_path = os.path.abspath(local_path)
        if os.listdir(local_path):
            return max([os.path.join(base_path, i) for i in os.listdir(local_path) if re.match(pattern, i)], key=os.path.getctime)

    @staticmethod
    def get_latest_file_from_weblist(webfilelist):
        """
           Takes List of Remote file objects and sort it by last modified date
           Return latest file object
        """
        if webfilelist is None:
            raise AssertionError('List of HttpFiles is "{}"'.format(webfilelist))
        if webfilelist:
            fileobj = sorted(webfilelist, key=lambda tstamp: (tstamp.date), reverse=True)
            return fileobj[0]

    @staticmethod
    def __filechunks(filesize, nsplits):
        """
            Split file to chunks
        """
        chunks = list()
        for i in range(nsplits):
            if i == 0:
                chunks.append('bytes=%s-%s' % (i, int(round(1 + i * filesize / (nsplits * 1.0) + filesize / (nsplits * 1.0) - 1, 0))))
            elif i == (nsplits - 1):
                chunks.append('bytes=%s-' % (int(round(1 + i * filesize / (nsplits * 1.0)))))
            else:
                chunks.append('bytes=%s-%s' % (int(round(1 + i * filesize / (nsplits * 1.0), 0)), int(round(1 + i * filesize / (nsplits * 1.0) + filesize / (nsplits * 1.0) - 1, 0))))
        return chunks

    def httpserver_open_connection(self, url, username=None, password=None):
        """
           Open Connection to Web Server
        """
        self.base_url = url
        self.session = requests.session()
        # Webserver doesnot require any authentication
        if username and password:
            self.session.auth = (username, password)
        self.session.headers['Content-Type'] = 'application/json'
        resp = self.session.get(self.base_url)
        if resp.status_code not in [200, 202, 206]:
            raise Exception("Not logged into HTTP SERVER . Error code: {}".format(resp.status_code))
        # Fetch Server type to handle different patterns of directory listing
        self.httpserver_get_server_type()

    @connected
    def httpserver_get_server_type(self):
        """
           Get Server Type
        """
        resp = self.session.head(self.base_url)
        logger.info("SERVER TYPE :{}".format(resp.headers['Server']), also_console=True)
        if 'Server' in resp.headers:
            if 'Apache' in resp.headers['Server']:
                self.parentdirectory = r'Parent Directory'
                return 'APACHE'
            elif 'Microsoft' in resp.headers['Server']:
                self.parentdirectory = r'[To Parent Directory]'
                return 'MICROSOFT'
            elif 'nginx' in resp.headers['Server']:
                pass
        return 'UNKNOWN'

    @connected
    def http_directory_download_file(self, remotefile, localfile, numsplits=2):
        """
            Download single file in chunks from web url
        """
        # Variable to split given remote file to number of chunks
        numsplits = int(numsplits)
        webpage = self.session.head(remotefile)
        if webpage.status_code not in [200, 202, 206]:
            raise Exception(webpage.status_code)
        # Get the file size
        size = int(webpage.headers['Content-Length'])
        # Divide single file to number of chunks
        t_chunk = self.__filechunks(size, numsplits)
        block = size // numsplits
        t_start = time.clock()
        threadlist = []
        for num in xrange(len(t_chunk)):
            tread = threading.Thread(target=self.__get_file_chunks, args=(remotefile, localfile, t_chunk[num], num))
            proc_num = {'proc': tread, 'file': localfile + str(num), 'size': block}
            if num == (len(t_chunk) - 1):
                proc_num['size'] = (size - (block * (len(t_chunk) - 1)))
            threadlist.append(proc_num)
        # Start all the threads in parallel
        for tred in threadlist:
            tred['proc'].start()
        # Wait until all threads are finished
        for tred in threadlist:
            tred['proc'].join()
        with open(localfile, 'wb') as outfile:
            for tread in threadlist:
                with open(tread['file'], 'rb') as infile:
                    copyfileobj(infile, outfile)
        # Remove the chunk files
        for tread in threadlist:
            os.remove(tread['file'])
        t_elapsed = time.clock() - t_start
        logger.info("Download of file '{0}' Completed in {1} secs".format(localfile, t_elapsed), also_console=True)

    @connected
    def helper_thread(self, args):
        """
        Helper Thread to run function in parallel
        """
        return self.http_directory_download_file(*args)

    @connected
    def http_directory_download_files(self, webfilelist, localpath, threadlimit=2):
        """
        Download multiple files in parallel using ThreadPool
        """
        workers = ThreadPool(threadlimit)
        arglist = []
        while True:
            if webfilelist:
                fileurl = webfilelist.pop()
                filename = fileurl.rsplit('/', 1)[-1]
                localfile = localpath + '//' + filename
                if os.path.isfile(localfile):
                    # Skip if file already exists in local path
                    logger.info("File '{}' already exists in local path".format(filename), also_console=True)
                    continue
                arglist.append((fileurl, localfile))
            else:
                break
        workers.map(self.helper_thread, arglist)
        workers.close()
        workers.join()

    @connected
    def http_directory_list_files(self, url):
        """
        Returns List of Files object properties from given Web url
        """
        # Build URL
        tablecolumns = ['Name', 'Last modified', 'Size', 'Description']
        list_files = []
        tableheaders = []
        weburl = url
        listresp = self.session.get(url, stream=True)
        if listresp.status_code not in [200, 202, 206]:
            raise Exception("Not Logged into Http Server.Error Code:{}".format(listresp.status_code))
        try:
            # Parse Response text to fetch all data from 'a' tag and ignore table headers if exists
            if listresp.text:
                data = html.fromstring(listresp.text)
                tablehead = data.xpath('.//a')
                tableheaders = [head.get('href') for head in tablehead if head.text in tablecolumns]
                listhrefs = data.xpath(".//a[text() != '" + self.parentdirectory + "']/@href")
                files = [os.path.basename(l) for l in listhrefs if l not in tableheaders]
                for filename in files:
                    webfile = self.HttpFile(filename)
                    webfile.url = weburl + "/" + filename
                    webpage = self.session.head(webfile.url)
                    if webpage.status_code not in [200, 202, 206]:
                        raise Exception(webpage.status_code)
                    # Remove directories from the link
                    if webfile.url.endswith('/'):
                        continue
                    if 'Content-Type' in webpage.headers:
                        pass
                    webfile.size = webpage.headers['Content-Length']
                    webfile.date = parse(webpage.headers['Last-Modified'], ignoretz=True)
                    list_files.append(webfile)
            else:
                raise Exception(listresp.text)
        except Exception as e_msg:
            msg = "Exception occurred while attempting to GET %s" % url
            raise Exception(msg, e_msg)
        return list_files

    @staticmethod
    def get_last_modified_date(folder, regex_filter='.*'):
        """
        Get Last Modified Date for the folder
        """
        dates = [datetime.datetime.fromtimestamp(os.path.getmtime(os.path.join(folder, x))) for x in filter(lambda y: re.match(regex_filter, y), os.listdir(folder))]
        return max(dates) if len(dates) > 0 else None

    @connected
    def is_http_file_latest(self, webfileobj, localfile):
        """
        Compare local file and remote file object based on its name and size.
        Input Parameters: absolute path of local file, remotefile object
        Returns boolean value
        """
        try:
            if os.path.exists(localfile):
                lsize = os.path.getsize(localfile)
                local_path = os.path.dirname(localfile)
                localfiledate = self.get_last_modified_date(local_path)
                # If file names are same, then check if remote file is newer or different size
                if webfileobj.name == os.path.basename(localfile):
                    if str(webfileobj.date) < str(localfiledate) or (str(webfileobj.size) == str(lsize)):
                        return False
        except Exception as e_msg:
            raise AssertionError('Exception occurred while comparing http file %s' % e_msg)
        return True

    @connected
    def __get_file_chunks(self, remotefile, localfile, byterange, num):
        """
        Download remote file in chunks
        """
        chunks = 1024 * 1024
        try:
            localfile += str(num)
            headers = {'Range': byterange}
            resp = self.session.get(remotefile, headers=headers, stream=True, verify=False)
            if resp.status_code not in [200, 202, 206]:
                raise Exception(resp.status_code)
            with open(localfile, 'wb') as infile:
                for chunk in resp.iter_content(chunk_size=chunks):
                    if chunk:  # filter out keep-alive new chunks
                        infile.write(chunk)
                        infile.flush()
            return resp
        except Exception as e_msg:
            msg = "Exception occurred while attempting to GET %s" % remotefile
            raise Exception(msg, e_msg)

    @connected
    def httpserver_close_connection(self):
        """
        Close Connection to Web Server
        """
        self.session.close()
        self.session = None


class HttpTests(unittest.TestCase):

    """
    Unit Test Cases
    """
    client = None
    system = "wpstwork4.vse.rdlabs.hpecorp.net"
    # resourceUrl = 'http://ci-artifacts02.vse.rdlabs.hpecorp.net/Fusion/rel/4.10/OVA/DCS'
    resourceUrl = 'http://wpstwork4.vse.rdlabs.hpecorp.net/OVST/SPP/DD-TH1'  # SPP2017100.2017_0908.53.iso'
    #resourceUrl = 'http://wpstwork4.vse.rdlabs.hpecorp.net/Files%20from%20Roopa/'
    # resourceUrl = 'http://robogalaxy.us.rdlabs.hpecorp.net/repo' # RoboGalaxyLibrary-300-cp27-win_amd64'
    # resourceUrl = 'http://ci-nexus.vse.rdlabs.hpecorp.net/Fusion/rel/3.10/OVA/NoSSH/'
    username = 'Web Url UserName'
    password = 'Web Url Password'
    localpath = 'C:\\test'
    numsplits = '1'

    def __init__(self, *args, **kwargs):
        super(HttpTests, self).__init__(*args, **kwargs)

    @classmethod
    def setUpClass(cls):
        cls.client = HttpDirectoryKeywords()
        cls.client.httpserver_open_connection(cls.resourceUrl, cls.username, cls.password)

    def test_webserver_connection(self):
        """
        Close Connection
        """
        self.client.httpserver_open_connection(self.resourceUrl, self.username, self.password)

    def test_singlefile_download_weburl(self):
        """
        Download single file from fileobject passed as input to function
        """
        files = self.client.http_directory_list_files(self.resourceUrl)
        # Get Latest Modified File
        newfile = self.client.get_latest_file_from_weblist(files)
        if newfile:
            remotefile_url = newfile.url
            remotefilename = newfile.name
            localfile = self.localpath + '\\' + remotefilename
            flag = self.client.is_http_file_latest(newfile, localfile)
            if flag:
                self.client.http_directory_download_file(remotefile_url, localfile, self.numsplits)
            else:
                pass
        else:
            raise Exception("File is not retrieved from HTTP directory", newfile)
        self.client.httpserver_close_connection()

    def test_multiple_downloads_weburl(self):
        """
        Download multiple files from given list
        """
        self.client.httpserver_open_connection(self.resourceUrl, self.username, self.password)
        files = self.client.http_directory_list_files(self.resourceUrl)
        if files:
            url_list = [t_file.url for t_file in files]
            self.client.http_directory_download_files(url_list, self.localpath, threadlimit=3)
        else:
            raise Exception("Files does not exists in Http Directory")


if __name__ == "__main__":
    unittest.main()
